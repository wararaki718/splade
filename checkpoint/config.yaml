config:
  lr: 2.0e-05
  seed: 123
  gradient_accumulation_steps: 1
  weight_decay: 0.01
  validation_metrics:
  - MRR@10
  - recall@100
  - recall@200
  - recall@500
  pretrained_no_yamlconfig: false
  nb_iterations: 150000
  train_batch_size: 64
  eval_batch_size: 600
  index_retrieve_batch_size: 500
  record_frequency: 10000
  train_monitoring_freq: 500
  warmup_steps: 6000
  max_length: 256
  fp16: true
  matching_type: splade
  monitoring_ckpt: MRR@10
  tokenizer_type: tohoku-nlp/bert-base-japanese-v3
  top_k: 1000
  threshold: 0
  eval_metric:
  - - mrr_10
    - recall
  retrieval_name:
  - MSMARCO
  loss: DistilMarginMSE
  regularizer:
    FLOPS:
      lambda_q: 0.1
      lambda_d: 0.08
      T: 50000
      targeted_rep: rep
      reg: FLOPS
  checkpoint_dir: ./checkpoint
  index_dir: models/japanese/index
  out_dir: models/japanese/out
data:
  type: triplets
  TRAIN_DATA_DIR: data/bclavie/mmarco-japanese-hard-negatives
  VALIDATION_SIZE_FOR_LOSS: 60000
  VALIDATION_FULL_RANKING:
    D_COLLECTION_PATH: data/mmarco/google/collections
    Q_COLLECTION_PATH: data/mmarco/google/queries/full_dev
    QREL_PATH: data/msmarco/val_retrieval/qrel.json
    TOP_K: 500
  COLLECTION_PATH: data/mmarco/google/collections
  Q_COLLECTION_PATH:
  - data/mmarco/google/queries/dev
  EVAL_QREL_PATH:
  - data/msmarco/dev_qrel.json
  flops_queries: data/mmarco/google/queries/dev
init_dict:
  model_type_or_dir: tohoku-nlp/bert-base-japanese-v3
  model_type_or_dir_q: null
  freeze_d_model: 0
  agg: max
  fp16: true
